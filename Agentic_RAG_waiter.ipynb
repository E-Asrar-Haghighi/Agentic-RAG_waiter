{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1JfqImf9DKu"
      },
      "source": [
        "# Libraries, API key, Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5EblfaX8GZe",
        "outputId": "83dbf77b-e6ff-4a3a-dde4-217433e13673"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m81.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q langchain-community langchain-openai unstructured faiss-cpu langgraph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Z6RRnfb-HZe",
        "outputId": "ef241b1d-1268-4661-caca-3885d0a06dfa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/GenAI/RAG/Agentic RAG\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/GenAI/RAG/Agentic RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LDFCE-bn-bb4",
        "outputId": "2ff9148a-0d10-45b4-bd0b-26b0d2e1e26f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "BSKCiOG0-bY-"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "api_key = userdata.get('genai_course')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "KtnNgqmF-bVd"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import UnstructuredExcelLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores.faiss import FAISS\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "\n",
        "from typing import List\n",
        "from typing_extensions import TypedDict\n",
        "from IPython.display import display, Image\n",
        "\n",
        "from langgraph.graph import StateGraph, END"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "f5jac0c_-bR0"
      },
      "outputs": [],
      "source": [
        "# Specify the path to the Excel file containing the menu\n",
        "file = 'dim sum montijo.xlsx'\n",
        "\n",
        "# Create an instance of the loader with the specified mode\n",
        "loader = UnstructuredExcelLoader(file, mode=\"elements\")\n",
        "\n",
        "# Load the data from the Excel file\n",
        "data = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "SVYOgfoD-bOL"
      },
      "outputs": [],
      "source": [
        "# Initialize the embeddings model with the OpenAI API key\n",
        "embeddings = OpenAIEmbeddings(openai_api_key=api_key)\n",
        "\n",
        "# Create a vector store (FAISS) from the documents using the embeddings\n",
        "db = FAISS.from_documents(data, embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGD39hTBGfDX"
      },
      "source": [
        "# Process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JbJvphlyGeYl"
      },
      "outputs": [],
      "source": [
        "# TypedDict class to store information\n",
        "class AgentState(TypedDict):\n",
        "    start: bool\n",
        "    conversation: int\n",
        "    question: str\n",
        "    answer: str\n",
        "    topic: bool\n",
        "    documents: list\n",
        "    recursion_limit: int\n",
        "    memory: list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EY6qVAShHz1Q"
      },
      "outputs": [],
      "source": [
        "def greetings(state):\n",
        "    # Greet the customer\n",
        "    print(\"Hello! Welcome to the restaurant. I will be your waiter. How can I help you?\")\n",
        "\n",
        "    # Capture user input\n",
        "    user_input = input()\n",
        "\n",
        "    # Update the state with the customer's question and initialize conversation variables\n",
        "    state['question'] = user_input\n",
        "    state['conversation'] = 1\n",
        "    state['memory'] = [user_input]\n",
        "\n",
        "    return state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gRQKlhuIH6WK"
      },
      "outputs": [],
      "source": [
        "def check_question(state):\n",
        "    # Get the customer's question from the state\n",
        "    question = state['question']\n",
        "\n",
        "    # Define the system prompt to evaluate the question's appropriateness\n",
        "    system_prompt = \"\"\"\n",
        "    You are a grader evaluating the appropriateness of a customer's question to a waiter or waitress in a restaurant.\n",
        "    Assess if the question is suitable to ask the restaurant staff and if the customer shows interest in continuing the conversation.\n",
        "    Respond with \"True\" if the question is appropriate for the staff or indicates the customer is asking a question or giving you information.\n",
        "    Otherwise respond with \"False\".\n",
        "    Provide only \"True\" or \"False\" in your response.\n",
        "    \"\"\"\n",
        "\n",
        "    # Create a prompt template for formatting\n",
        "    TEMPLATE = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", system_prompt),\n",
        "        (\"human\", \"User question: {question}\"),\n",
        "    ])\n",
        "\n",
        "    # Format the prompt with the customer's question\n",
        "    prompt = TEMPLATE.format(question=question)\n",
        "\n",
        "    # Initialize the language model\n",
        "    model = ChatOpenAI(model=\"gpt-4o-mini\", api_key=api_key)\n",
        "\n",
        "    # Invoke the model with the prompt\n",
        "    response_text = model.invoke(prompt)\n",
        "\n",
        "    # Update the state with the model's decision\n",
        "    state['topic'] = response_text.content.strip()\n",
        "\n",
        "    return state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fShDQE9OH6Sm"
      },
      "outputs": [],
      "source": [
        "# Function to check if the LLM model decides if the question is on topic on top\n",
        "def topic_router(state):\n",
        "  topic = state['topic']\n",
        "  if topic == \"True\":\n",
        "    return \"on_topic\"\n",
        "  else:\n",
        "    return \"off_topic\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cpl5sJYONDFq"
      },
      "outputs": [],
      "source": [
        "def off_topic_response(state):\n",
        "    # Check if it's the first conversation turn\n",
        "    if state['conversation'] <= 1:\n",
        "        state['answer'] = \"\\nI apologize, I can't answer that question. I can only answer questions about the menu in this restaurant.\"\n",
        "        print(state['answer'])\n",
        "    else:\n",
        "        state['answer'] = \"\\nHappy to help.\"\n",
        "        print(state['answer'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MMJjMRuXNDCI"
      },
      "outputs": [],
      "source": [
        "def retrieve_docs(state):\n",
        "    # Combine the conversation history into a single string\n",
        "    memory = \" \".join(state['memory'])\n",
        "\n",
        "    # Retrieve the top 5 relevant documents based on similarity to the conversation history\n",
        "    docs_faiss = db.similarity_search(memory, k=5)\n",
        "\n",
        "    # Store the retrieved documents' content in the state\n",
        "    state['documents'] = [doc.page_content for doc in docs_faiss]\n",
        "\n",
        "    return state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gPdQdjqrNC--"
      },
      "outputs": [],
      "source": [
        "def generate(state):\n",
        "    # Initialize the language model\n",
        "    model = ChatOpenAI(model=\"gpt-4o-mini\", api_key=api_key)\n",
        "\n",
        "    # Extract necessary information from the state\n",
        "    question = state['question']\n",
        "    documents = state['documents']\n",
        "    memory = state['memory']\n",
        "\n",
        "    # Define the system prompt for the waiter\n",
        "    system_prompt = \"\"\"\n",
        "    You are a waiter at a restaurant tasked with answering customer's questions about the menu.\n",
        "    Answer the question in the manner of a waiter, avoiding being too verbose or too brief.\n",
        "    Do not include \"waiter\" or refer to yourself explicitly in your answer.\n",
        "    \"\"\"\n",
        "\n",
        "    # Create a prompt template for formatting\n",
        "    TEMPLATE = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", system_prompt),\n",
        "        (\"human\", \"Context: {documents}\\nConversation history so far: {memory}\\nCustomer question: {question}\"),\n",
        "    ])\n",
        "\n",
        "    # Format the prompt with the retrieved documents, conversation history, and customer's question\n",
        "    prompt = TEMPLATE.format(documents=documents, memory=memory, question=question)\n",
        "\n",
        "    # Invoke the model to generate an answer\n",
        "    response_text = model.invoke(prompt)\n",
        "\n",
        "    # Store the generated answer in the state\n",
        "    state[\"answer\"] = response_text.content.strip()\n",
        "\n",
        "    return state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GcfuETEeNC7d"
      },
      "outputs": [],
      "source": [
        "def improve_answer(state):\n",
        "    # Extract necessary information from the state\n",
        "    question = state['question']\n",
        "    answer = state['answer']\n",
        "    memory = state['memory']\n",
        "\n",
        "    # Define the system prompt for refining the answer\n",
        "    system = \"\"\"\n",
        "    As a waiter, review and refine the response to a customer's question. Your task is to:\n",
        "\n",
        "    1. Ensure the answer is appropriate, friendly, and informative.\n",
        "    2. Edit or remove parts of the answer as needed, without adding new information.\n",
        "    3. Maintain a polite, professional, and attentive tone.\n",
        "    4. Provide only the improved answer, without any introductory phrases or commentary.\n",
        "    5. Conclude the response with an open-ended question to invite further inquiries or address additional needs.\n",
        "    6. Consider the conversation history to be more informative and useful.\n",
        "    7. Include line breaks (`\\n`) at the end of each sentence or logical break.\n",
        "\n",
        "    Deliver a refined response that enhances the customer's experience and reflects the restaurant's commitment to customer service.\n",
        "    \"\"\"\n",
        "\n",
        "    # Create a prompt template for formatting\n",
        "    TEMPLATE = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", system),\n",
        "        (\"human\", \"Customer question: {question}\\nConversation history: {memory}\\nWaiter's initial answer: {answer}\"),\n",
        "    ])\n",
        "\n",
        "    # Initialize the language model\n",
        "    model = ChatOpenAI(model=\"gpt-4o-mini\", api_key=api_key)\n",
        "\n",
        "    # Format the prompt with the necessary information\n",
        "    prompt = TEMPLATE.format(question=question, memory=memory, answer=answer)\n",
        "\n",
        "    # Invoke the model to improve the answer\n",
        "    response_text = model.invoke(prompt)\n",
        "\n",
        "    # Update the state with the improved answer\n",
        "    state['answer'] = response_text.content.strip()\n",
        "\n",
        "    # Display the improved answer\n",
        "    print('\\n')\n",
        "    print(state['answer'])\n",
        "\n",
        "    # Append the improved answer to the conversation history\n",
        "    state['memory'].append(state['answer'])\n",
        "\n",
        "    return state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zndv7-HZH6PB"
      },
      "outputs": [],
      "source": [
        "def further_question(state):\n",
        "    # Prompt the customer for further input\n",
        "    print('\\n')\n",
        "    user_input = input()\n",
        "\n",
        "    # Update the state with the new question and increment the conversation turn\n",
        "    state['question'] = user_input\n",
        "    state['conversation'] += 1\n",
        "    state['memory'].append(user_input)\n",
        "\n",
        "    return state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Workflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize a StateGraph with the AgentState type\n",
        "workflow = StateGraph(AgentState)\n",
        "\n",
        "\n",
        "# Add the functions as nodes in the workflow\n",
        "workflow.add_node(\"greetings\", greetings)\n",
        "workflow.add_node(\"check_question\", check_question)\n",
        "workflow.add_node(\"off_topic_response\", off_topic_response)\n",
        "workflow.add_node(\"retrieve_docs\", retrieve_docs)\n",
        "workflow.add_node(\"generate\", generate)\n",
        "workflow.add_node(\"improve_answer\", improve_answer)\n",
        "workflow.add_node(\"further_question\", further_question)\n",
        "\n",
        "# Set the entry point of the workflow to the greetings function\n",
        "workflow.set_entry_point(\"greetings\")\n",
        "\n",
        "# Add conditional edges based on the topic_router function's output\n",
        "workflow.add_conditional_edges(\n",
        "    \"check_question\",\n",
        "    topic_router,\n",
        "    {\n",
        "        \"on_topic\": \"retrieve_docs\",\n",
        "        \"off_topic\": \"off_topic_response\"\n",
        "    }\n",
        ")\n",
        "\n",
        "# Define the sequence of steps in the workflow\n",
        "workflow.add_edge(\"greetings\", \"check_question\")\n",
        "workflow.add_edge(\"retrieve_docs\", \"generate\")\n",
        "workflow.add_edge(\"generate\", \"improve_answer\")\n",
        "workflow.add_edge(\"improve_answer\", \"further_question\")\n",
        "workflow.add_edge(\"further_question\", \"check_question\")\n",
        "\n",
        "# Connect the off_topic_response node to the end of the workflow\n",
        "workflow.add_edge(\"off_topic_response\", END)\n",
        "\n",
        "# Compile the workflow into an application\n",
        "app = workflow.compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display the workflow graph for visualization\n",
        "display(Image(app.get_graph(xray=True).draw_mermaid_png()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 896
        },
        "id": "KKQPKTPtSJ0L",
        "outputId": "00545627-3511-4723-9781-83967fdc89f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello! Welcome to the restaurant. I will be your waiter. How can I help you?\n",
            "best dish for a very hungry man\n",
            "\n",
            "\n",
            "For a very hungry man, I recommend the Arroz em Lótus, featuring sticky rice with chicken, shrimp, and mushrooms.  \n",
            "\n",
            "It's a hearty dish that should satisfy any big appetite.  \n",
            "\n",
            "If you're looking for something to share, the Siao Long Pao or our Gyoza selection are also great options, as they are both flavorful and filling!  \n",
            "\n",
            "Is there anything else I can assist you with or any other preferences you have?\n",
            "\n",
            "\n",
            "how much dose it cost?\n",
            "\n",
            "\n",
            "The Arroz em Lótus costs €5.30.  \n",
            "\n",
            "The Siao Long Pao and Gyozas are priced at €4.50 each.  \n",
            "\n",
            "If you'd like more information on other dishes or need assistance with your order, please feel free to ask!\n",
            "\n",
            "\n",
            "what drink do you recommend with it?\n",
            "\n",
            "\n",
            "For pairing with the Arroz em Lótus, I recommend a glass of Lello White or Red wine, as it complements the dish beautifully.  \n",
            "\n",
            "If you prefer a non-alcoholic option, our Cold Tea selections, like the White Tea with Mandarin and Mint, offer a refreshing balance.  \n",
            "\n",
            "Would you like to try one of these options, or is there something else I can assist you with?\n",
            "\n",
            "\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-249497d29f05>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Invoke the application with the initial state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"start\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"recursion_limit\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langgraph/pregel/__init__.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, **kwargs)\u001b[0m\n\u001b[1;32m   2365\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2366\u001b[0m             \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2367\u001b[0;31m         for chunk in self.stream(\n\u001b[0m\u001b[1;32m   2368\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2369\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langgraph/pregel/__init__.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[1;32m   2022\u001b[0m                 \u001b[0;31m# with channel updates applied only at the transition between steps.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2023\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtick\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_channels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2024\u001b[0;31m                     for _ in runner.tick(\n\u001b[0m\u001b[1;32m   2025\u001b[0m                         \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2026\u001b[0m                         \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langgraph/pregel/runner.py\u001b[0m in \u001b[0;36mtick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m                 run_with_retry(\n\u001b[0m\u001b[1;32m    231\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m                     \u001b[0mretry_policy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langgraph/pregel/retry.py\u001b[0m in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;31m# run the task\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mParentCommand\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mns\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCONF\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCONFIG_KEY_CHECKPOINT_NS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langgraph/utils/runnable.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    544\u001b[0m                 )\n\u001b[1;32m    545\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 546\u001b[0;31m                     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    547\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m                     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langgraph/utils/runnable.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m             \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_set_config_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRunnable\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-a1ba0a34206c>\u001b[0m in \u001b[0;36mfurther_question\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# Prompt the customer for further input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# Update the state with the new question and increment the conversation turn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "# Invoke the application with the initial state\n",
        "result = app.invoke({\"start\": True}, {\"recursion_limit\": 50})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ClnVE60GAUI2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t3ZXA2tCAUFM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pFAoRnSZAUAW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJ-zM_t3AT9R"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eSOatRulAT6G"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JDiKvqZnAT2n"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PncQodRfATy1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wPRUc05-ATuz"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
